{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksplorasi Data & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi Data: Memahami Data dengan Statistik - Part 1\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('https://storage.googleapis.com/dqlab-dataset/pythonTutorial/online_raw.csv')\n",
    "print('Shape dataset:', dataset.shape)\n",
    "print('\\nLima data teratas:\\n', dataset.head())\n",
    "print('\\nInformasi dataset:')\n",
    "print(dataset.info())\n",
    "print('\\nStatistik deskriptif:\\n', dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi Data: Memahami Data dengan Statistik - Part 2\n",
    "dataset_corr = dataset.corr()\n",
    "print('Korelasi dataset:\\n', dataset.corr())\n",
    "print('Distribusi Label (Revenue):\\n', dataset['Revenue'].value_counts())\n",
    "# Tugas praktek\n",
    "print('\\nKorelasi BounceRates-ExitRates:', dataset_corr.loc['BounceRates', 'ExitRates'])\n",
    "print('\\nKorelasi Revenue-PageValues:', dataset_corr.loc['Revenue', 'PageValues'])\n",
    "print('\\nKorelasi TrafficType-Weekend:', dataset_corr.loc['TrafficType', 'Weekend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi Data: Memahami Data dengan Visual\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# checking the Distribution of customers on Revenue\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(dataset['Revenue'], palette = 'pastel')\n",
    "plt.title('Buy or Not', fontsize = 20)\n",
    "plt.xlabel('Revenue or not', fontsize = 14)\n",
    "plt.ylabel('count', fontsize = 14)\n",
    "# checking the Distribution of customers on Weekend\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(dataset['Weekend'], palette = 'inferno')\n",
    "plt.title('Purchase on Weekends', fontsize = 20)\n",
    "plt.xlabel('Weekend or not', fontsize = 14)\n",
    "plt.ylabel('count', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tugas Praktek\n",
    "import matplotlib.pyplot as plt\n",
    "# visualizing the distribution of customers around the Region\n",
    "plt.hist(dataset['region'], color = 'lightblue')\n",
    "plt.title('Distribution of Customers', fontsize = 20)\n",
    "plt.xlabel('Region Codes', fontsize = 14)\n",
    "plt.ylabel('Count Users', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing: Handling Missing Value - Part 1\n",
    "#checking missing value for each feature  \n",
    "print('Checking missing value for each feature:')\n",
    "print(dataset.isnull().sum())\n",
    "#Counting total missing value\n",
    "print('\\nCounting total missing value:')\n",
    "print(dataset.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing: Handling Missing Value - Part 2\n",
    "#Drop rows with missing value   \n",
    "dataset_clean = dataset.dropna()\n",
    "print('Ukuran dataset_clean:', dataset_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing: Handling Missing Value - Part 3\n",
    "print(\"Before imputation:\")\n",
    "# Checking missing value for each feature  \n",
    "print(dataset.isnull().sum())\n",
    "# Counting total missing value  \n",
    "print(dataset.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nAfter imputation:\")\n",
    "# Fill missing value with mean of feature value  \n",
    "dataset.fillna(dataset.mean(), inplace = True)\n",
    "# Checking missing value for each feature  \n",
    "print(dataset.isnull().sum())\n",
    "# Counting total missing value  \n",
    "print(dataset.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tugas Praktek\n",
    "import pandas as pd\n",
    "dataset1 = pd.read_csv('https://storage.googleapis.com/dqlab-dataset/pythonTutorial/online_raw.csv')\n",
    "\n",
    "print(\"Before imputation:\")\n",
    "# Checking missing value for each feature  \n",
    "print(dataset1.isnull().sum())\n",
    "# Counting total missing value  \n",
    "print(dataset1.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nAfter imputation:\")\n",
    "# Fill missing value with median of feature value  \n",
    "dataset1.fillna(dataset1.median(), inplace = True)\n",
    "# Checking missing value for each feature  \n",
    "print(dataset1.isnull().sum())\n",
    "# Counting total missing value  \n",
    "print(dataset1.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing: Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Define MinMaxScaler as scaler  \n",
    "scaler = MinMaxScaler()  \n",
    "#list all the feature that need to be scaled  \n",
    "scaling_column = ['Administrative','Administrative_Duration','Informational','Informational_Duration','ProductRelated','ProductRelated_Duration','BounceRates','ExitRates','PageValues']\n",
    "#Apply fit_transfrom to scale selected feature  \n",
    "dataset[scaling_column] = scaler.fit_transform(dataset[scaling_column])\n",
    "#Cheking min and max value of the scaling_column\n",
    "print(dataset[scaling_column].describe().T[['min','max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing: Konversi string ke numerik\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Convert feature/column 'Month'\n",
    "LE = LabelEncoder()\n",
    "dataset['Month'] = LE.fit_transform(dataset['Month'])\n",
    "print(LE.classes_)\n",
    "print(np.sort(dataset['Month'].unique()))\n",
    "print('')\n",
    "\n",
    "# Convert feature/column 'VisitorType'\n",
    "LE = LabelEncoder()\n",
    "dataset['VisitorType'] = LE.fit_transform(dataset['VisitorType'])\n",
    "print(LE.classes_)\n",
    "print(np.sort(dataset['VisitorType'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemodelan dengan Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features & Label\n",
    "# removing the target column Revenue from dataset and assigning to X\n",
    "X = dataset.drop(['Revenue'], axis = 1)\n",
    "# assigning the target column Revenue to y\n",
    "y = dataset['Revenue']\n",
    "# checking the shapes\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dan Test Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# splitting the X, and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "# checking the shapes\n",
    "print(\"Shape of X_train :\", X_train.shape)\n",
    "print(\"Shape of y_train :\", y_train.shape)\n",
    "print(\"Shape of X_test :\", X_test.shape)\n",
    "print(\"Shape of y_test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model: Fit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Call the classifier\n",
    "model = DecisionTreeClassifier()\n",
    "# Fit the classifier to the training data\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model: Predict\n",
    "# Apply the classifier/model to the test data\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi Model Performance - Part 2\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# evaluating the model\n",
    "print('Training Accuracy :', model.score(X_train, y_train))\n",
    "print('Testing Accuracy :', model.score(X_test, y_test))\n",
    "\n",
    "# confusion matrix\n",
    "print('\\nConfusion matrix:')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "print('\\nClassification report:')\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pemodelan Permasalahan Klasifikasi dengan Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Call the classifier\n",
    "logreg = LogisticRegression()\n",
    "# Fit the classifier to the training data  \n",
    "logreg = logreg.fit(X_train, y_train)\n",
    "#Training Model: Predict \n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#Evaluate Model Performance\n",
    "print('Training Accuracy :', model.score(X_train, y_train))  \n",
    "print('Testing Accuracy :', model.score(X_test, y_test))  \n",
    "\n",
    "# confusion matrix\n",
    "print('\\nConfusion matrix')  \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# classification report  \n",
    "print('\\nClassification report')  \n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification - Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Call the classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "# Fit the classifier to the training data\n",
    "decision_tree = decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# evaluating the decision_tree performance\n",
    "print('Training Accuracy :', decision_tree.score(X_train, y_train))\n",
    "print('Testing Accuracy :', decision_tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression: Linear Regression\n",
    "#load dataset\n",
    "import pandas as pd\n",
    "housing = pd.read_csv('https://storage.googleapis.com/dqlab-dataset/pythonTutorial/housing_boston.csv')\n",
    "#Data rescaling\n",
    "from sklearn import preprocessing\n",
    "data_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "housing[['RM', 'LSTAT', 'PTRATIO', 'MEDV']] = data_scaler.fit_transform(housing[['RM', 'LSTAT', 'PTRATIO', 'MEDV']])\n",
    "# getting dependent and independent variables\n",
    "X = housing.drop(['MEDV'], axis = 1)\n",
    "y = housing['MEDV']\n",
    "# checking the shapes\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of y:', y.shape)\n",
    "\n",
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "# checking the shapes\n",
    "print('Shape of X_train :', X_train.shape)\n",
    "print('Shape of y_train :', y_train.shape)\n",
    "print('Shape of X_test :', X_test.shape)\n",
    "print('Shape of y_test :', y_test.shape)\n",
    "\n",
    "##import regressor from Scikit-Learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Call the regressor\n",
    "reg = LinearRegression()\n",
    "# Fit the regressor to the training data\n",
    "reg = reg.fit(X_train,y_train)\n",
    "# Apply the regressor/model to the test data\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression: Performance Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Calculating MSE, lower the value better it is. 0 means perfect prediction\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean squared error of testing set:', mse)\n",
    "#Calculating MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('Mean absolute error of testing set:', mae)\n",
    "#Calculating RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print('Root Mean Squared Error of testing set:', rmse)\n",
    "\n",
    "#Plotting y_test dan y_pred\n",
    "plt.scatter(y_test, y_pred, c = 'green')\n",
    "plt.xlabel('Price Actual')\n",
    "plt.ylabel('Predicted value')\n",
    "plt.title('True value vs predicted value : Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering\n",
    "#import library\n",
    "import pandas as pd  \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#load dataset\n",
    "dataset = pd.read_csv('https://storage.googleapis.com/dqlab-dataset/pythonTutorial/mall_customers.csv')\n",
    "\n",
    "#selecting features  \n",
    "X = dataset[['annual_income', 'spending_score']]  \n",
    "\n",
    "#Define KMeans as cluster_model  \n",
    "cluster_model = KMeans(n_clusters = 5, random_state = 24)  \n",
    "labels = cluster_model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering - Inspect & Visualizing the Cluster\n",
    "#import library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#convert dataframe to array\n",
    "X = X.values\n",
    "#Separate X to xs and ys --> use for chart axis\n",
    "xs = X[:,0]\n",
    "ys = X[:,1]\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs, ys, c = labels, alpha = 0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "centroids = cluster_model.cluster_centers_\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x, centroids_y, marker = 'D', s = 50)\n",
    "plt.title('K Means Clustering', fontsize = 20)\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring Cluster Criteria\n",
    "#import library\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Elbow Method - Inertia plot\n",
    "inertia = []\n",
    "#looping the inertia calculation for each k\n",
    "for k in range(1, 10):\n",
    "    #Assign KMeans as cluster_model\n",
    "    cluster_model = KMeans(n_clusters = k, random_state = 24)\n",
    "    #Fit cluster_model to X\n",
    "    cluster_model.fit(X)\n",
    "    #Get the inertia value\n",
    "    inertia_value = cluster_model.inertia_\n",
    "    #Append the inertia_value to inertia list\n",
    "    inertia.append(inertia_value)\n",
    "    \n",
    "##Inertia plot\n",
    "plt.plot(range(1, 10), inertia)\n",
    "plt.title('The Elbow Method - Inertia plot', fontsize = 20)\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study: Promos for our e-commerce - Part 1\n",
    "#import library \n",
    "import pandas as pd\n",
    "\n",
    "# Baca data 'ecommerce_banner_promo.csv'\n",
    "data = pd.read_csv('https://storage.googleapis.com/dqlab-dataset/pythonTutorial/ecommerce_banner_promo.csv')\n",
    "\n",
    "#1. Data eksplorasi dengan head(), info(), describe(), shape\n",
    "print(\"\\n[1] Data eksplorasi dengan head(), info(), describe(), shape\")\n",
    "print(\"Lima data teratas:\")\n",
    "print(data.head())\n",
    "print(\"Informasi dataset:\")\n",
    "print(data.info())\n",
    "print(\"Statistik deskriptif dataset:\")\n",
    "print(data.describe())\n",
    "print(\"Ukuran dataset:\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study: Promos for our e-commerce - Part 2\n",
    "#2. Data eksplorasi dengan dengan mengecek korelasi dari setiap feature menggunakan fungsi corr()\n",
    "print(\"\\n[2] Data eksplorasi dengan dengan mengecek korelasi dari setiap feature menggunakan fungsi corr()\")\n",
    "print(data.corr())\n",
    "\n",
    "#3. Data eksplorasi dengan mengecek distribusi label menggunakan fungsi groupby() dan size()\n",
    "print(\"\\n[3] Data eksplorasi dengan mengecek distribusi label menggunakan fungsi groupby() dan size()\")\n",
    "print(data.groupby('Clicked on Ad').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study: Promos for our e-commerce - Part 3\n",
    "#import library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seting: matplotlib and seaborn\n",
    "sns.set_style('whitegrid')  \n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#4. Data eksplorasi dengan visualisasi\n",
    "#4a. Visualisasi Jumlah user dibagi ke dalam rentang usia (Age) menggunakan histogram (hist()) plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data['Age'], bins = data.Age.nunique())\n",
    "plt.xlabel('Age')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#4b. Gunakan pairplot() dari seaborn (sns) modul untuk menggambarkan hubungan setiap feature.\n",
    "plt.figure()\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study: Promos for our e-commerce - Part 4\n",
    "#5. Cek missing value\n",
    "print(\"\\n[5] Cek missing value\")\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study: Promos for our e-commerce - Part 5\n",
    "#import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#6.Lakukan pemodelan dengan Logistic Regression, gunakan perbandingan 80:20 untuk training vs testing\n",
    "print(\"\\n[6] Lakukan pemodelan dengan Logistic Regression, gunakan perbandingan 80:20 untuk training vs testing\")\n",
    "#6a.Drop Non-Numerical (object type) feature from X, as Logistic Regression can only take numbers, and also drop Target/label, assign Target Variable to y.   \n",
    "X = data.drop(['Ad Topic Line','City','Country','Timestamp','Clicked on Ad'], axis = 1)\n",
    "y = data['Clicked on Ad']\n",
    "\n",
    "#6b. splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "#6c. Modelling\n",
    "# Call the classifier\n",
    "logreg = LogisticRegression()\n",
    "# Fit the classifier to the training data\n",
    "logreg = logreg.fit(X_train, y_train)\n",
    "# Prediksi model\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#6d. Evaluasi Model Performance\n",
    "print(\"Evaluasi Model Performance:\")\n",
    "print(\"Training Accuracy :\", logreg.score(X_train, y_train))\n",
    "print(\"Testing Accuracy :\", logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study: Promos for our e-commerce - Part 6\n",
    "# Import library\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#7. Print Confusion matrix dan classification report\n",
    "print(\"\\n[7] Print Confusion matrix dan classification report\")\n",
    "\n",
    "#apply confusion_matrix function to y_test and y_pred\n",
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "#apply classification_report function to y_test and y_pred\n",
    "print(\"Classification report:\")\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
